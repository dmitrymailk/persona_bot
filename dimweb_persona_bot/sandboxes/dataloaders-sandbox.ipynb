{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid'])\n",
      "train 17878\n",
      "dict_keys(['personality', 'utterances'])\n",
      "valid 1000\n",
      "dict_keys(['personality', 'utterances'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# read the json file \n",
    "with open('./datasets/persona_chat.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(data.keys())\n",
    "    for key in data.keys():\n",
    "        print(key, len(data[key]))\n",
    "        print(data[key][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i watch basketball .',\n",
       " 'i go to a local college .',\n",
       " 'i work at a smoothie shop .',\n",
       " 'i listen to classic rock .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][12]['personality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lengths: train 17878, valid 500, test 500\n",
      "Datasets saved.\n"
     ]
    }
   ],
   "source": [
    "from dimweb_persona_bot.datasets_transformers.persona_chat_dataset_transformer import persona_chat_dataset_tranformer_v1\n",
    "\n",
    "persona_chat_dataset_tranformer_v1(\n",
    "    initial_dataset_path=\"./datasets/persona_chat/persona_chat.json\",\n",
    "    output_folder=\"./datasets/persona_chat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey how are you today ?',\n",
       " 'great ! just go off work at the smoothie shop . you ?',\n",
       " 'i have been eating tacos and getting ready to move to school .',\n",
       " 'are you going to college ? i go to a local one .',\n",
       " 'yes , i am going to university of michigan . what year are you ?',\n",
       " 'first yr ! do you have any hobbies ?',\n",
       " 'i love doing anything outdoors . especially in summer . you ?',\n",
       " 'i love watching college basketball and rocking out to classic rock .',\n",
       " 'fun . have you decided on your major for school ?',\n",
       " 'not yet . have you decided ?',\n",
       " \"pre med . i'd love to be a doctor\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# —á–µ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ history —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–µ—Ä—Å–æ–Ω—É\n",
    "data['train'][12]['utterances'][-1]['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['my mom is my best friend .',\n",
       "  'i have four sisters .',\n",
       "  'i believe that mermaids are real .',\n",
       "  'i love iced tea .'],\n",
       " 'history': ['hi , how are you doing today ?',\n",
       "  'i am spending time with my 4 sisters what are you up to',\n",
       "  'wow , four sisters . just watching game of thrones .',\n",
       "  'that is a good show i watch that while drinking iced tea',\n",
       "  'i agree . what do you do for a living ?',\n",
       "  \"i'm a researcher i'm researching the fact that mermaids are real\",\n",
       "  \"interesting . i'm a website designer . pretty much spend all my time on the computer .\",\n",
       "  \"that's cool my mom does the same thing\",\n",
       "  \"that's awesome . i have always had a love for technology .\",\n",
       "  'tell me more about yourself',\n",
       "  'i really enjoy free diving , how about you , have any hobbies ?']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dimweb_persona_bot.dataloaders.persona_chat_dataloaders import PersonaChatDatasetV1\n",
    "dataset = PersonaChatDatasetV1(\n",
    "    input_dataset_path=\"./datasets/persona_chat/train.json\",\n",
    ")\n",
    "dataset[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 15332,   287,  ...,  -100,  -100,  -100],\n",
       "         [50256,  1820,  4004,  ...,  -100,  -100,  -100],\n",
       "         [50256,    72,   588,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [50256,    72,  1842,  ...,  -100,  -100,  -100],\n",
       "         [50256,    72,  1842,  ...,  -100,  -100,  -100],\n",
       "         [50256,    72,   588,  ...,  -100,  -100,  -100]]),\n",
       " 'labels': tensor([[50256, 15332,   287,  ...,  -100,  -100,  -100],\n",
       "         [50256,  1820,  4004,  ...,  -100,  -100,  -100],\n",
       "         [50256,    72,   588,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [50256,    72,  1842,  ...,  -100,  -100,  -100],\n",
       "         [50256,    72,  1842,  ...,  -100,  -100,  -100],\n",
       "         [50256,    72,   588,  ...,  -100,  -100,  -100]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dimweb_persona_bot.dataloaders.persona_chat_dataloaders import PersonaChatDatasetV1\n",
    "from dimweb_persona_bot.dataloaders.causal_samplers import CausalTrainPersonaSampleV1, CausalValidPersonaSampleV1\n",
    "from dimweb_persona_bot.dataloaders.lighting import LightningDataModuleV1\n",
    "from dimweb_persona_bot.hyperparameters.causal_modeling_hyperparameters import (\n",
    "    PersonaChatHyperparametersV1,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "hyperparameters = PersonaChatHyperparametersV1()\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparameters.model_name)\n",
    "\n",
    "lighting_data = LightningDataModuleV1(\n",
    "\ttrain_path_dataset=\"./datasets/persona_chat/train.json\",\n",
    "\tvalid_path_dataset=\"./datasets/persona_chat/valid.json\",\n",
    "\thyperparameters=hyperparameters,\n",
    "\ttokenizer=tokenizer,\n",
    "\tbase_train_dataset_class=PersonaChatDatasetV1,\n",
    "\tbase_valid_dataset_class=PersonaChatDatasetV1,\n",
    "\tbase_train_sample_class=CausalTrainPersonaSampleV1,\n",
    "\tbase_valid_sample_class=CausalValidPersonaSampleV1,\n",
    ")\n",
    "lighting_data.setup()\n",
    "next(iter(lighting_data.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dimweb_persona_bot.dataloaders.persona_chat_dataloaders import PersonaChatDatasetV1\n",
    "\n",
    "\n",
    "train_dataset = PersonaChatDatasetV1(\n",
    "    input_dataset_path=\"./datasets/persona_chat/train.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['i like to snowboard .',\n",
       "  'my favorite food is popcorn .',\n",
       "  'i like to ride horses .',\n",
       "  'i live in rural wisconsin .'],\n",
       " 'history': ['i am frank . nice to meet you . what is your name ?',\n",
       "  'my name is gary . great to meet you too .',\n",
       "  'i work as a general manager at a grocery store . what about you ?',\n",
       "  \"i'm an insurance salesman\"],\n",
       " 'sample_id': '15_2'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = PersonaChatDatasetV1(\n",
    "    input_dataset_path=\"./datasets/persona_chat/valid.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['i read twenty books a year .',\n",
       "  \"i'm a stunt double as my second job .\",\n",
       "  'i only eat kosher .',\n",
       "  'i was raised in a single parent household .'],\n",
       " 'history': ['hello what are doing today ?',\n",
       "  'i am good , i just got off work and tired , i have two jobs .'],\n",
       " 'sample_id': '0_1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello what are doing today ?',\n",
       " 'i am good , i just got off work and tired , i have two jobs .',\n",
       " 'i just got done watching a horror movie',\n",
       " \"i rather read , i've read about 20 books this year .\",\n",
       " 'wow ! i do love a good horror movie . loving this cooler weather',\n",
       " 'but a good movie is always good .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[2]['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow ! i do love a good horror movie . loving this cooler weather',\n",
       " 'but a good movie is always good .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[2]['history'][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ru persona chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>characteristic_1</th>\n",
       "      <th>characteristic_2</th>\n",
       "      <th>characteristic_3</th>\n",
       "      <th>characteristic_4</th>\n",
       "      <th>characteristic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç–∞.</th>\n",
       "      <td>–Ø —É–≤–∞–∂–∞—é –ª—é–¥–µ–π.</td>\n",
       "      <td>–£ –º–µ–Ω—è –µ—Å—Ç—å –∂–∏–≤–æ—Ç–Ω–æ–µ.</td>\n",
       "      <td>–£ –º–µ–Ω—è —Ö–æ—Ä–æ—à–∏–π –¥—Ä—É–≥.</td>\n",
       "      <td>–Ø –ª—é–±–ª—é –∫–æ—Ñ–µ.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º</th>\n",
       "      <td>–£ –º–µ–Ω—è –µ—Å—Ç—å —Å–æ–±–∞–∫–∞</td>\n",
       "      <td>–Ø –ª—é–±–ª—é –ø–µ—Ç—å</td>\n",
       "      <td>–Ø –∂–∏–≤—É —Å–∞–º–∞</td>\n",
       "      <td>–Ø –ª—é–±–ª—é —Ü–≤–µ—Ç—ã</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º</th>\n",
       "      <td>–Ø –±–µ–≥–∞—é –ø–æ —É—Ç—Ä–∞–º</td>\n",
       "      <td>–Ø —Ä–∞–±–æ—Ç–∞—é –Ω–∞ —Ä–∞–±–æ—Ç–µ</td>\n",
       "      <td>–Ø –ø–æ–µ–¥—É –≤ –æ—Ç–ø—É—Å–∫</td>\n",
       "      <td>–Ø –ª—é–±–ª—é –∞—Ä–±—É–∑</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç</th>\n",
       "      <td>—É –º–µ–Ω—è —Ç—Ä–æ–µ –¥–µ—Ç–µ–π</td>\n",
       "      <td>–Ω–µ –ª—é–±–ª—é —Å–≤–æ—é —Ä–∞–±–æ—Ç—É</td>\n",
       "      <td>–Ω—Ä–∞–≤–∏—Ç—å—Å—è –µ–∑–¥–∏—Ç—å –Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–µ</td>\n",
       "      <td>–ª—é–±–ª—é –ø–∏–≤–æ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.</th>\n",
       "      <td>–Ø –µ—â—ë —É—á—É—Å—å.</td>\n",
       "      <td>–ù–æ —è –º–µ—á—Ç–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å.</td>\n",
       "      <td>–Ø –æ–±–æ–∂–∞—é —Ä–æ–¥–∏—Ç–µ–ª–µ–π.</td>\n",
       "      <td>–ò –Ω–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          characteristic_1       characteristic_2  \\\n",
       "–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç–∞.     –Ø —É–≤–∞–∂–∞—é –ª—é–¥–µ–π.  –£ –º–µ–Ω—è –µ—Å—Ç—å –∂–∏–≤–æ—Ç–Ω–æ–µ.   \n",
       "–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º      –£ –º–µ–Ω—è –µ—Å—Ç—å —Å–æ–±–∞–∫–∞           –Ø –ª—é–±–ª—é –ø–µ—Ç—å   \n",
       "–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º              –Ø –±–µ–≥–∞—é –ø–æ —É—Ç—Ä–∞–º    –Ø —Ä–∞–±–æ—Ç–∞—é –Ω–∞ —Ä–∞–±–æ—Ç–µ   \n",
       "—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç           —É –º–µ–Ω—è —Ç—Ä–æ–µ –¥–µ—Ç–µ–π   –Ω–µ –ª—é–±–ª—é —Å–≤–æ—é —Ä–∞–±–æ—Ç—É   \n",
       "–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.                  –Ø –µ—â—ë —É—á—É—Å—å.  –ù–æ —è –º–µ—á—Ç–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å.   \n",
       "\n",
       "                                      characteristic_3     characteristic_4  \\\n",
       "–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç–∞.            –£ –º–µ–Ω—è —Ö–æ—Ä–æ—à–∏–π –¥—Ä—É–≥.        –Ø –ª—é–±–ª—é –∫–æ—Ñ–µ.   \n",
       "–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º                         –Ø –∂–∏–≤—É —Å–∞–º–∞        –Ø –ª—é–±–ª—é —Ü–≤–µ—Ç—ã   \n",
       "–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º                          –Ø –ø–æ–µ–¥—É –≤ –æ—Ç–ø—É—Å–∫        –Ø –ª—é–±–ª—é –∞—Ä–±—É–∑   \n",
       "—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç          –Ω—Ä–∞–≤–∏—Ç—å—Å—è –µ–∑–¥–∏—Ç—å –Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–µ           –ª—é–±–ª—é –ø–∏–≤–æ   \n",
       "–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.                       –Ø –æ–±–æ–∂–∞—é —Ä–æ–¥–∏—Ç–µ–ª–µ–π.  –ò –Ω–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è.   \n",
       "\n",
       "                        characteristic_5  \n",
       "–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç–∞.               NaN  \n",
       "–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º                   NaN  \n",
       "–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º                         NaN  \n",
       "—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç                       NaN  \n",
       "–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.                         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"./datasets/ru_persona_chat/profiles.tsv\", delimiter=\"\\t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persona_1_profile</th>\n",
       "      <th>persona_2_profile</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;span class=participant_1&gt;–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–ò—â—É –ø—Ä–∏–Ω—Ü–∞.&lt;br /&gt;–í–µ–¥...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;span class=participant_1&gt;–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º&lt;b...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–Ø –±–∏–∑–Ω–µ—Å–º–µ–Ω&lt;br /&gt;–£ –º...</td>\n",
       "      <td>&lt;span class=participant_1&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;span class=participant_1&gt;–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º&lt;br /&gt;–Ø ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–Ø –ø–æ—é –≤ –∫–∞—Ä–∞–æ–∫–µ&lt;br /...</td>\n",
       "      <td>&lt;span class=participant_1&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;span class=participant_1&gt;—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç&lt;br /&gt;...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–Ø –º–∞–ª—å—á–∏–∫&lt;br /&gt;–Ø —É—á—É...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ó–¥—Ä–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;span class=participant_1&gt;–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.&lt;br /&gt;–Ø ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;–Ø –ø—Ä–æ—Å—Ç–æ–≤–∞—Ç.&lt;br /&gt;–õ—é...</td>\n",
       "      <td>&lt;span class=participant_1&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   persona_1_profile  \\\n",
       "0  <span class=participant_1>–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç...   \n",
       "1  <span class=participant_1>–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º<b...   \n",
       "2  <span class=participant_1>–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º<br />–Ø ...   \n",
       "3  <span class=participant_1>—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç<br />...   \n",
       "4  <span class=participant_1>–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.<br />–Ø ...   \n",
       "\n",
       "                                   persona_2_profile  \\\n",
       "0  <span class=participant_2>–ò—â—É –ø—Ä–∏–Ω—Ü–∞.<br />–í–µ–¥...   \n",
       "1  <span class=participant_2>–Ø –±–∏–∑–Ω–µ—Å–º–µ–Ω<br />–£ –º...   \n",
       "2  <span class=participant_2>–Ø –ø–æ—é –≤ –∫–∞—Ä–∞–æ–∫–µ<br /...   \n",
       "3  <span class=participant_2>–Ø –º–∞–ª—å—á–∏–∫<br />–Ø —É—á—É...   \n",
       "4  <span class=participant_2>–Ø –ø—Ä–æ—Å—Ç–æ–≤–∞—Ç.<br />–õ—é...   \n",
       "\n",
       "                                            dialogue  \n",
       "0  <span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤...  \n",
       "1  <span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...  \n",
       "2  <span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...  \n",
       "3  <span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ó–¥—Ä–∞...  \n",
       "4  <span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./datasets/ru_persona_chat/dialogues.tsv\", delimiter=\"\\t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=participant_1>–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.<br />–Ø –µ—â—ë —É—á—É—Å—å.<br />–ù–æ —è –º–µ—á—Ç–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å.<br />–Ø –æ–±–æ–∂–∞—é —Ä–æ–¥–∏—Ç–µ–ª–µ–π.<br />–ò –Ω–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è.<br /></span>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[4]['persona_1_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=participant_2>–Ø –ø—Ä–æ—Å—Ç–æ–≤–∞—Ç.<br />–õ—é–¥–∏ –∏–∑–±–µ–≥–∞—é—Ç –º–µ–Ω—è.<br />–Ø –±—ã—Å—Ç—Ä–æ –±–µ–≥–∞—é.<br />–ú–æ–∏ —É–≤–ª–µ—á–µ–Ω–∏—è –Ω–µ–æ—Ä–¥–∏–Ω–∞—Ä–Ω—ã.<br />–Ø —Ä–∞–±–æ—Ç–∞—é –ø–æ –ø—Ä–∏–∑–≤–∞–Ω–∏—é.<br /></span>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[4]['persona_2_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç!</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç!</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ù–æ—Ä–º–∞–ª—å–Ω–æ, –≥–æ—Ç–æ–≤–ª—é—Å—å –∫–æ —Å–Ω—É. –ó–∞–≤—Ç—Ä–∞ —Å–Ω–æ–≤–∞ –≤ —à–∫–æ–ª—É<br />. –ù–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è.</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —Ç–≤–æ–∏ –∫–∞–∫?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –í—Å—ë —Ö–æ—Ä–æ—à–æ,—Å–ø–∞—Ç—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è,–¥—É–º–∞—é —Ñ–∏–ª—å–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ö–∞–∫–æ–π —Ñ–∏–ª—å–º?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ï—â—ë –Ω–µ —Ä–µ—à–∏–ª–∞, –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–µ—Ç–µ–∫—Ç–∏–≤ –∫–∞–∫–æ–π –Ω–∏–±—É–¥—å. –ê<br />–∫–∞–∫ –≤ —à–∫–æ–ª–µ —É —Ç–µ–±—è?</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –•–æ—Ä–æ—à–æ, –µ—â—ë —É—á—É—Å—å, –Ω–æ —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á—É. –£–∂–µ –º–µ—á—Ç–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å<br />, –∞ –Ω–µ —Å–∏–¥–µ—Ç—å –∑–∞ —É—á–µ–±–Ω–∏–∫–∞–º–∏. –ê —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –∏–ª–∏ —É—á–∏—à—å—Å—è<br />–µ—â—ë?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è —Ä–∞–±–æ—Ç–∞—é, –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –º–æ—è —Ä–∞–±–æ—Ç–∞, –∫–µ–º –ø–ª–∞–Ω–∏—Ä—É–µ—à—å<br />—Ä–∞–±–æ—Ç–∞—Ç—å?</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –•–æ—á—É –±—ã—Ç—å –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º. –ê –∫–µ–º —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ù–µ –ø–æ–≤–µ—Ä–∏—à—å....—è –ø—Å–∏—Ö–æ–ª–æ–≥</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ö—Ä—É—Ç–æ! –ò –∫–∞–∫ —Ç–µ–±–µ?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è,—è –ª—é–±–ª—é —Å–≤–æ—é —Ä–∞–±–æ—Ç—É.</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –≠—Ç–æ –∫–ª–∞—Å—Å–Ω–æ. –•–æ—á—É —Ç–∞–∫–∂–µ. –ê —Ç—ã –æ–¥–Ω–∞ –∂–∏–≤–µ—à—å?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ù–µ—Ç,—è –∂–∏–≤—É —Å –º–∞–º–æ–π,–∞ —Ç—ã?</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –Ø —Ç–æ–∂–µ —Å —Ä–æ–¥–∏—Ç–µ–ª—è–º–∏ –∂–∏–≤—É, –æ–±–æ–∂–∞—é –∏—Ö. –ù–æ –∏–Ω–æ–≥–¥–∞ —Ö–æ—á–µ—Ç—Å—è<br />–ø–æ–∂–∏—Ç—å –æ–¥–Ω–æ–π.</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ú–Ω–µ —Ç–æ–∂–µ, —è –ª—é–±–ª—é –±—ã—Ç—å –¥–æ–º–∞ –æ–¥–Ω–∞, –º–Ω–µ —ç–∏–æ —á–∞—Å—Ç–æ —É–¥–∞—ë—Ç—Å—è<br />, –º—ã —Å –º–∞–º–æ–π —Ä–∞–±–æ—Ç–∞–µ–º –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è, –∞ –≤ –∫–∞–∫–æ–º –≥–æ—Ä–æ–¥–µ<br />—Ç—ã –∂–∏–≤—ë—à—å?</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —É –º–µ–Ω—è –º–∞–º–∞ –¥–æ–º–æ—Ö–æ–∑—è–π–∫–∞, –ø–æ—ç—Ç–æ–º—É —Ä–µ–¥–∫–æ –±—ã–≤–∞—é –¥–æ–º–∞<br />–æ–¥–Ω–∞. –í –ü–∏—Ç–µ—Ä–µ, –∞ —Ç—ã?</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è –≤ –†–æ—Å—Ç–æ–≤–µ –Ω–∞ –î–æ–Ω—É.</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –•–æ—á—É —Ç–∞–º –ø–æ–±—ã–≤–∞—Ç—å.</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –õ–∞–¥–Ω–æ, –º–Ω–µ –ø–æ—Ä–∞ –ª–æ–∂–∏—Ç—å—Å—è</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –†–∞–¥–∞ –±—ã–ª–∞ –ø–æ–±–æ–ª—Ç–∞—Ç—å!</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü–æ–∑–¥–Ω–æ —É–∂–µ –º–æ–∂–µ—Ç –¥–æ –∑–∞–≤—Ç—Ä–∞? –°–ü–û–ö–û–ô–ù–û–ô –ù–û–ß–ò, –ü–†–ò–ï–ó–ñ–ê–ô<br />–í –ì–û–°–¢–ò</span><br />'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[4]['dialogue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset.iloc[4]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç!\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç!\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ù–æ—Ä–º–∞–ª—å–Ω–æ, –≥–æ—Ç–æ–≤–ª—é—Å—å –∫–æ —Å–Ω—É. –ó–∞–≤—Ç—Ä–∞ —Å–Ω–æ–≤–∞ –≤ —à–∫–æ–ª—É\n",
      " <br/>\n",
      " . –ù–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è.\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —Ç–≤–æ–∏ –∫–∞–∫?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –í—Å—ë —Ö–æ—Ä–æ—à–æ,—Å–ø–∞—Ç—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è,–¥—É–º–∞—é —Ñ–∏–ª—å–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ö–∞–∫–æ–π —Ñ–∏–ª—å–º?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ï—â—ë –Ω–µ —Ä–µ—à–∏–ª–∞, –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–µ—Ç–µ–∫—Ç–∏–≤ –∫–∞–∫–æ–π –Ω–∏–±—É–¥—å. –ê\n",
      " <br/>\n",
      " –∫–∞–∫ –≤ —à–∫–æ–ª–µ —É —Ç–µ–±—è?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –•–æ—Ä–æ—à–æ, –µ—â—ë —É—á—É—Å—å, –Ω–æ —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á—É. –£–∂–µ –º–µ—á—Ç–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å\n",
      " <br/>\n",
      " , –∞ –Ω–µ —Å–∏–¥–µ—Ç—å –∑–∞ —É—á–µ–±–Ω–∏–∫–∞–º–∏. –ê —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –∏–ª–∏ —É—á–∏—à—å—Å—è\n",
      " <br/>\n",
      " –µ—â—ë?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è —Ä–∞–±–æ—Ç–∞—é, –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –º–æ—è —Ä–∞–±–æ—Ç–∞, –∫–µ–º –ø–ª–∞–Ω–∏—Ä—É–µ—à—å\n",
      " <br/>\n",
      " —Ä–∞–±–æ—Ç–∞—Ç—å?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –•–æ—á—É –±—ã—Ç—å –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º. –ê –∫–µ–º —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ù–µ –ø–æ–≤–µ—Ä–∏—à—å....—è –ø—Å–∏—Ö–æ–ª–æ–≥\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ö—Ä—É—Ç–æ! –ò –∫–∞–∫ —Ç–µ–±–µ?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è,—è –ª—é–±–ª—é —Å–≤–æ—é —Ä–∞–±–æ—Ç—É.\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –≠—Ç–æ –∫–ª–∞—Å—Å–Ω–æ. –•–æ—á—É —Ç–∞–∫–∂–µ. –ê —Ç—ã –æ–¥–Ω–∞ –∂–∏–≤–µ—à—å?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ù–µ—Ç,—è –∂–∏–≤—É —Å –º–∞–º–æ–π,–∞ —Ç—ã?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –Ø —Ç–æ–∂–µ —Å —Ä–æ–¥–∏—Ç–µ–ª—è–º–∏ –∂–∏–≤—É, –æ–±–æ–∂–∞—é –∏—Ö. –ù–æ –∏–Ω–æ–≥–¥–∞ —Ö–æ—á–µ—Ç—Å—è\n",
      " <br/>\n",
      " –ø–æ–∂–∏—Ç—å –æ–¥–Ω–æ–π.\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ú–Ω–µ —Ç–æ–∂–µ, —è –ª—é–±–ª—é –±—ã—Ç—å –¥–æ–º–∞ –æ–¥–Ω–∞, –º–Ω–µ —ç–∏–æ —á–∞—Å—Ç–æ —É–¥–∞—ë—Ç—Å—è\n",
      " <br/>\n",
      " , –º—ã —Å –º–∞–º–æ–π —Ä–∞–±–æ—Ç–∞–µ–º –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è, –∞ –≤ –∫–∞–∫–æ–º –≥–æ—Ä–æ–¥–µ\n",
      " <br/>\n",
      " —Ç—ã –∂–∏–≤—ë—à—å?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —É –º–µ–Ω—è –º–∞–º–∞ –¥–æ–º–æ—Ö–æ–∑—è–π–∫–∞, –ø–æ—ç—Ç–æ–º—É —Ä–µ–¥–∫–æ –±—ã–≤–∞—é –¥–æ–º–∞\n",
      " <br/>\n",
      " –æ–¥–Ω–∞. –í –ü–∏—Ç–µ—Ä–µ, –∞ —Ç—ã?\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è –≤ –†–æ—Å—Ç–æ–≤–µ –Ω–∞ –î–æ–Ω—É.\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –•–æ—á—É —Ç–∞–º –ø–æ–±—ã–≤–∞—Ç—å.\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –õ–∞–¥–Ω–æ, –º–Ω–µ –ø–æ—Ä–∞ –ª–æ–∂–∏—Ç—å—Å—è\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –†–∞–¥–∞ –±—ã–ª–∞ –ø–æ–±–æ–ª—Ç–∞—Ç—å!\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_1\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏\n",
      "</span>\n",
      "<br/>\n",
      "<span class=\"participant_2\">\n",
      " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü–æ–∑–¥–Ω–æ —É–∂–µ –º–æ–∂–µ—Ç –¥–æ –∑–∞–≤—Ç—Ä–∞? –°–ü–û–ö–û–ô–ù–û–ô –ù–û–ß–ò, –ü–†–ò–ï–ó–ñ–ê–ô\n",
      " <br/>\n",
      " –í –ì–û–°–¢–ò\n",
      "</span>\n",
      "<br/>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(dialogue)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ò—â—É –ø—Ä–∏–Ω—Ü–∞.',\n",
       " '–í–µ–¥—É –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏.',\n",
       " '–õ—é–±–ª—é —á–∏—Ç–∞—Ç—å –∫–ª–∞—Å—Å–∏–∫—É.',\n",
       " '–í—ã—Ä–∞—â–∏–≤–∞—é —Ñ–∏–∞–ª–∫–∏.',\n",
       " '–õ—é–±–ª—é –æ–±—â–µ–Ω–∏–µ.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = dataset.iloc[0]['persona_2_profile']\n",
    "soup2 = BeautifulSoup(profile)\n",
    "[item + \".\" for item in soup2.text.split(\".\") if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('span'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('span')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['participant_2']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('span')[0].get('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ù–æ—Ä–º–∞–ª—å–Ω–æ, –≥–æ—Ç–æ–≤–ª—é—Å—å –∫–æ —Å–Ω—É. –ó–∞–≤—Ç—Ä–∞ —Å–Ω–æ–≤–∞ –≤ —à–∫–æ–ª—É<br/>. –ù–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([str(item) for item in soup.find_all('span')[3].contents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_1  –ü—Ä–∏–≤–µ—Ç!\n",
      "participant_2  –ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?\n",
      "participant_1  –ù–æ—Ä–º–∞–ª—å–Ω–æ, –≥–æ—Ç–æ–≤–ª—é—Å—å –∫–æ —Å–Ω—É. –ó–∞–≤—Ç—Ä–∞ —Å–Ω–æ–≤–∞ –≤ —à–∫–æ–ª—É. –ù–µ –ª—é–±–ª—é —É—á–∏—Ç—å—Å—è. –ê —Ç–≤–æ–∏ –∫–∞–∫?\n",
      "participant_2  –í—Å—ë —Ö–æ—Ä–æ—à–æ,—Å–ø–∞—Ç—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è,–¥—É–º–∞—é —Ñ–∏–ª—å–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å\n",
      "participant_1  –ö–∞–∫–æ–π —Ñ–∏–ª—å–º?\n",
      "participant_2  –ï—â—ë –Ω–µ —Ä–µ—à–∏–ª–∞, –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–µ—Ç–µ–∫—Ç–∏–≤ –∫–∞–∫–æ–π –Ω–∏–±—É–¥—å. –ê–∫–∞–∫ –≤ —à–∫–æ–ª–µ —É —Ç–µ–±—è?\n",
      "participant_1  –•–æ—Ä–æ—à–æ, –µ—â—ë —É—á—É—Å—å, –Ω–æ —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á—É. –£–∂–µ –º–µ—á—Ç–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å, –∞ –Ω–µ —Å–∏–¥–µ—Ç—å –∑–∞ —É—á–µ–±–Ω–∏–∫–∞–º–∏. –ê —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –∏–ª–∏ —É—á–∏—à—å—Å—è–µ—â—ë?\n",
      "participant_2  –ê —è —Ä–∞–±–æ—Ç–∞—é, –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –º–æ—è —Ä–∞–±–æ—Ç–∞, –∫–µ–º –ø–ª–∞–Ω–∏—Ä—É–µ—à—å—Ä–∞–±–æ—Ç–∞—Ç—å?\n",
      "participant_1  –•–æ—á—É –±—ã—Ç—å –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º. –ê –∫–µ–º —Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å?\n",
      "participant_2  –ù–µ –ø–æ–≤–µ—Ä–∏—à—å....—è –ø—Å–∏—Ö–æ–ª–æ–≥\n",
      "participant_1  –ö—Ä—É—Ç–æ! –ò –∫–∞–∫ —Ç–µ–±–µ?\n",
      "participant_2  –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è,—è –ª—é–±–ª—é —Å–≤–æ—é —Ä–∞–±–æ—Ç—É.\n",
      "participant_1  –≠—Ç–æ –∫–ª–∞—Å—Å–Ω–æ. –•–æ—á—É —Ç–∞–∫–∂–µ. –ê —Ç—ã –æ–¥–Ω–∞ –∂–∏–≤–µ—à—å?\n",
      "participant_2  –ù–µ—Ç,—è –∂–∏–≤—É —Å –º–∞–º–æ–π,–∞ —Ç—ã?\n",
      "participant_1  –Ø —Ç–æ–∂–µ —Å —Ä–æ–¥–∏—Ç–µ–ª—è–º–∏ –∂–∏–≤—É, –æ–±–æ–∂–∞—é –∏—Ö. –ù–æ –∏–Ω–æ–≥–¥–∞ —Ö–æ—á–µ—Ç—Å—è–ø–æ–∂–∏—Ç—å –æ–¥–Ω–æ–π.\n",
      "participant_2  –ú–Ω–µ —Ç–æ–∂–µ, —è –ª—é–±–ª—é –±—ã—Ç—å –¥–æ–º–∞ –æ–¥–Ω–∞, –º–Ω–µ —ç–∏–æ —á–∞—Å—Ç–æ —É–¥–∞—ë—Ç—Å—è, –º—ã —Å –º–∞–º–æ–π —Ä–∞–±–æ—Ç–∞–µ–º –≤ —Ä–∞–∑–Ω–æ–µ –≤—Ä–µ–º—è, –∞ –≤ –∫–∞–∫–æ–º –≥–æ—Ä–æ–¥–µ—Ç—ã –∂–∏–≤—ë—à—å?\n",
      "participant_1  –ê —É –º–µ–Ω—è –º–∞–º–∞ –¥–æ–º–æ—Ö–æ–∑—è–π–∫–∞, –ø–æ—ç—Ç–æ–º—É —Ä–µ–¥–∫–æ –±—ã–≤–∞—é –¥–æ–º–∞–æ–¥–Ω–∞. –í –ü–∏—Ç–µ—Ä–µ, –∞ —Ç—ã?\n",
      "participant_2  –ê —è –≤ –†–æ—Å—Ç–æ–≤–µ –Ω–∞ –î–æ–Ω—É.\n",
      "participant_1  –•–æ—á—É —Ç–∞–º –ø–æ–±—ã–≤–∞—Ç—å. –õ–∞–¥–Ω–æ, –º–Ω–µ –ø–æ—Ä–∞ –ª–æ–∂–∏—Ç—å—Å—è –†–∞–¥–∞ –±—ã–ª–∞ –ø–æ–±–æ–ª—Ç–∞—Ç—å! –°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Replica(TypedDict):\n",
    "    text: str\n",
    "    persona_class: str\n",
    "\n",
    "dialogue = []\n",
    "replicas = soup.find_all('span')\n",
    "current_class = replicas[0].get('class')[0]\n",
    "\n",
    "def simple_filter(text: str) -> str:\n",
    "    text = text.replace(\"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1:\", \"\")\n",
    "    text = text.replace(\"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2:\", \"\")\n",
    "    return text\n",
    "\n",
    "current_text = \"\"\n",
    "for replica in replicas:\n",
    "    if replica.get('class')[0] == current_class:\n",
    "        current_text += simple_filter(replica.text)\n",
    "    else:\n",
    "        replica_obj = Replica(text=current_text, persona_class=current_class)\n",
    "        dialogue.append(replica_obj)\n",
    "        current_class = replica.get('class')[0]\n",
    "        current_text = simple_filter(replica.text)\n",
    "        \n",
    "\n",
    "for item in dialogue:\n",
    "    print(item['persona_class'], item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_samples = []\n",
    "\n",
    "dialogue_len = len(dialogue) // 2 \n",
    "for i in range(dialogue_len):\n",
    "\tsample = dialogue[:i*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574555/3331626809.py:1: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  pd.DataFrame(dialogue).to_dict('r')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': ' –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ', 'persona_class': 'participant_2'},\n",
       " {'text': ' –ü—Ä–∏–≤–µ—Ç) –ø–æ–¥ –≤–∫—É—Å–Ω—ã–π –∫–æ—Ñ–µ–µ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–±–æ–ª—Ç–∞—Ç—å –ø–æ—è–≤–∏–ª–æ—Å—å)',\n",
       "  'persona_class': 'participant_1'},\n",
       " {'text': ' –ß—Ç–æ —á–∏—Ç–∞–µ—à—å? –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏–∫–∞ –Ø —Ç–æ–∂–µ –ª—é–±–ª—é –ø–æ–æ–±—â–∞—Ç—å—Å—è',\n",
       "  'persona_class': 'participant_2'},\n",
       " {'text': ' –õ—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞—é, –∫–∞–∫ –∏ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É) –Ø —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫—É –ª—é–±–ª—é',\n",
       "  'persona_class': 'participant_1'},\n",
       " {'text': ' –ê —è –≤—ã—Ä–∞—â–∏–≤–∞—é —Ñ–∏–∞–ª–∫–∏ –ò –≤–µ–¥—É –∑–¥–æ—Ä–æ–≤—ã–π –∏ –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏!',\n",
       "  'persona_class': 'participant_2'},\n",
       " {'text': ' –£—Ö —Ç—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.', 'persona_class': 'participant_1'},\n",
       " {'text': ' –¢—ã —Å–ª—É—á–∞–π–Ω–æ –Ω–µ –ø—Ä–∏–Ω—Ü –Ω–∞ –±–µ–ª–æ–º –∫–æ–Ω–µ? –Ø –µ–≥–æ –æ—á–µ–Ω—å –∂–¥—É..',\n",
       "  'persona_class': 'participant_2'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dialogue).to_dict('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['–Ø —é—Ä–∏—Å—Ç.',\n",
       "  '–ù–µ –∑–∞–º—É–∂–µ–º.',\n",
       "  '–õ—é–±–ª—é —Ç–∞–Ω—Ü–µ–≤–∞—Ç—å, –ø–µ—Ç—å.',\n",
       "  '–ú–æ–µ —Ö–æ–±–±–∏ –∫—É–ª–∏–Ω–∞—Ä–∏—è.',\n",
       "  '–Ø –ª—é–±–ª—é –ª–µ—Ç–æ, –º–æ—Ä–µ, —Å–æ–ª–Ω—Ü–µ –∏ –ø–µ—Å–æ–∫.'],\n",
       " 'history': ['–ü—Ä–∏–≤–µ—Ç.',\n",
       "  '–ø—Ä–∏–≤–µ—Ç!',\n",
       "  '–ö–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç? –ò —á–µ–º —Ç—ã –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è?',\n",
       "  '–∞–Ω–Ω–∞ —è —é—Ä–∏—Å—Ç. —è –ª—é–±–ª—é —Ç–æ–Ω—Ü–µ–≤–∞—Ç—å –∏ –ø–µ—Ç—å. –∞ —Ç—ã —á–µ–º –ª—é–±–∏—à—å –∑–∞–Ω–∏–º–∞—Ç—å—Å—è ?',\n",
       "  '–Ø –û–ª—å–≥–∞! –î–æ–º–æ—Ö–æ–∑—è–π–∫–∞ –∏ –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥ –ø–æ —Å–æ–≤–º–µ—Å—Ç–∏—Ç–µ–ª—å—Å—Ç–≤—É üòÑ –ü–µ—á—å –ø–∏—Ä–æ–≥–∏ –∏ –≤—Å—ë —Ç–∞–∫–æ–µ) –¢—ã –æ—Ç –∫—É–¥–∞? –õ—é–±–∏—à—å –∂–∏–≤–æ—Ç–Ω—ã—Ö?',\n",
       "  '–±—Ä—è–Ω—Å–∫. –∞ —Ç—ã? –º–æ—ë —Ö–æ–±–±–∏ –∫—É–ª–∏–Ω–æ—Ä–∏—è. –¥–∞ –∫–æ–Ω–µ—à–Ω–æ. –º—ã –¥–∞–∂–µ —Å –º—É–∂–∞–º —Ä–∞–∑–≤–µ–ª–∏—Å—å –∏–∑ –∑–∞ –∫–æ—à–∫–∏ ) —Ç–µ—Ä–µ—Ä—å —è –Ω–µ –∑–∞–º—É–∂–µ–º)'],\n",
       " 'sample_id': '23_3'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dimweb_persona_bot.dataloaders.ru_persona_chat_dataloaders import RUPersonaChatDatasetV1\n",
    "from dimweb_persona_bot.datasets_transformers.ru_persona_chat_dataset_transformer import ru_persona_chat_dataset_tranformer_v1\n",
    "\n",
    "# ru_persona_chat_dataset_tranformer_v1(\n",
    "# \tinitial_dataset_path=\"./datasets/ru_persona_chat/dialogues.tsv\",\n",
    "# \toutput_folder=\"./datasets/ru_persona_chat\",\n",
    "# )\n",
    "\n",
    "train_dataset = RUPersonaChatDatasetV1(\n",
    "    input_dataset_path=\"./datasets/ru_persona_chat/valid.csv\",\n",
    ")\n",
    "train_dataset[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['–Ø –∂–µ–Ω–∞—Ç.',\n",
       "  '–Ø —Ä–∞–±–æ—Ç–∞—é –≤ –∞–≤—Ç–æ—Å–∞–ª–æ–Ω–µ.',\n",
       "  '–£ –º–µ–Ω—è –µ—Å—Ç—å –±–æ–ª—å—à–æ–π –¥–æ–º.',\n",
       "  '–Ø –º–µ—á—Ç–∞—é –æ –¥–µ—Ç—è—Ö.',\n",
       "  '–£ –º–µ–Ω—è –µ—Å—Ç—å —Å–≤–æ—è —Ñ–µ—Ä–º–∞.'],\n",
       " 'history': ['–ü—Ä–∏–≤–µ—Ç!', '–ü—Ä–∏–≤–µ—Ç.'],\n",
       " 'sample_id': '27_1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–Ø –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫.<br/>–Ø —Ä–∞–∑–≤–µ–¥–µ–Ω.<br/>–£ –º–µ–Ω—è –∫–∞—Ä–∏–µ –≥–ª–∞–∑–∞.<br/>–Ø –∏–≥—Ä–∞—é –Ω–∞ –±–∞—è–Ω–µ.<br/>–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–∞—á–∞.<br/> '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "persona = '<span class=\"participant_1\">—Ö–æ—á—É –∫–æ—à–∫—É<br/>–º–µ—á—Ç–∞—é –ø—Ä—ã–≥–Ω—É—Ç—å —Å –ø–∞—Ä–∞—à—é—Ç–æ–º<br/>–Ω–∞—É—á–∏–ª —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞—Ç—å –ø–æ–ø—É–≥–∞—è<br/>–ª—é–±–ª—é –ø—Ä–∏—Ä–æ–¥—É<br/>–æ–±–æ–∂–∞—é –±–∞–Ω—é<br/></span> '\n",
    "persona = '<span class=\"participant_2\">–Ø –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫.<br/>–Ø —Ä–∞–∑–≤–µ–¥–µ–Ω.<br/>–£ –º–µ–Ω—è –∫–∞—Ä–∏–µ –≥–ª–∞–∑–∞.<br/>–Ø –∏–≥—Ä–∞—é –Ω–∞ –±–∞—è–Ω–µ.<br/>–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–∞—á–∞.<br/></span> '\n",
    "# persona = persona.replace(\"<br/>\", \". \")\n",
    "\n",
    "# soup = BeautifulSoup(\n",
    "# \tpersona,\n",
    "# \tfeatures=\"html.parser\",\n",
    "# )\n",
    "# soup.text\n",
    "re.sub(r\"<span.*\\\">|</span>\", \"\", persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['<span class=\"participant_1\">—Ö–æ—á—É –∫–æ—à–∫—É<br/>–º–µ—á—Ç–∞—é –ø—Ä—ã–≥–Ω—É—Ç—å —Å –ø–∞—Ä–∞—à—é—Ç–æ–º<br/>–Ω–∞—É—á–∏–ª —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞—Ç—å –ø–æ–ø—É–≥–∞—è<br/>–ª—é–±–ª—é –ø—Ä–∏—Ä–æ–¥—É<br/>–æ–±–æ–∂–∞—é –±–∞–Ω—é<br/></span> '],\n",
       " 'history': ['–ü—Ä–∏–≤–µ—Ç.',\n",
       "  '–ü—Ä–∏–≤–µ—Ç.',\n",
       "  '–ö–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç? –ß–µ–º –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è –ø–æ –∂–∏–∑–Ω–∏?',\n",
       "  '–ú–∞—Ä–∏—è. –ñ–∏–≤—É –≤ –¥–µ—Ä–µ–≤–Ω–µ, –¥–µ—Ä–∂—É –ø–æ–ø—É–≥–∞—è –∏ —Ö–æ—á—É –∫–æ—à–∫—É.',\n",
       "  '–ö–∞–∫ –∑–¥–æ—Ä–æ–≤–æ, —è –±–µ–∑—É–º–Ω–æ –ª—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö. –£ –º–µ–Ω—è –µ—Å—Ç—å —Å–æ–±–∞–∫–∏ –∏ –ø–æ–ø—É–≥–∞–∏.',\n",
       "  '–ö–∞–∫–∏–µ —Å–æ–±–∞–∫–∏ –∏ —Å–∫–æ–ª—å–∫–æ?',\n",
       "  '–Ø –ª—é–±–ª—é –∏ –∫–æ—Ç–∏–∫–æ–≤,–Ω–æ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤–µ–ª–∞. –¢—Ä–∏ —Å–æ–±–∞–∫–∏,–¥–≤–∞ –ª–∞–±—Ä–∞–¥–æ—Ä–∞ –∏ —á–∞—É-—á–∞—É. –õ—é–±–ª—é –ø—É—à–∏—Å—Ç–∏–∫–æ–≤)',\n",
       "  '–Ø –ª—é–±–ª—é –ø—Ä–∏—Ä–æ–¥—É –∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö.',\n",
       "  '–Ø —Ç–æ–∂–µ.',\n",
       "  '–õ–∞–±—Ä–∞–¥–æ—Ä—ã –æ—á–µ–Ω—å –¥–æ–±—Ä—ã–µ —Å–æ–±–∞–∫–∏.'],\n",
       " 'sample_id': '10_5'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\", )\n",
    "\n",
    "len(tokenizer.encode(' '.join(train_dataset[14]['persona'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['<span class=\"participant_2\">–Ø –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫.<br/>–Ø —Ä–∞–∑–≤–µ–¥–µ–Ω.<br/>–£ –º–µ–Ω—è –∫–∞—Ä–∏–µ –≥–ª–∞–∑–∞.<br/>–Ø –∏–≥—Ä–∞—é –Ω–∞ –±–∞—è–Ω–µ.<br/>–£ –º–µ–Ω—è –µ—Å—Ç—å –¥–∞—á–∞.<br/></span> '],\n",
       " 'history': ['–ü—Ä–∏–≤–µ—Ç.', '–ü—Ä–∏–≤–µ—Ç —Ç—ã –∫—Ç–æ?'],\n",
       " 'sample_id': '12_1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "No valid references for a sentence!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdimweb_persona_bot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m TextEvaluator\n\u001b[1;32m      3\u001b[0m t_eval \u001b[39m=\u001b[39m TextEvaluator()\n\u001b[0;32m----> 5\u001b[0m t_eval\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m      6\u001b[0m \t[\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      7\u001b[0m \t[\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m/cephfs/home/kosenko/persona_bot/dimweb_persona_bot/utils.py:41\u001b[0m, in \u001b[0;36mTextEvaluator.evaluate\u001b[0;34m(self, generated_texts, original_texts)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     38\u001b[0m     generated_texts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     39\u001b[0m     original_texts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m     40\u001b[0m ):\n\u001b[0;32m---> 41\u001b[0m     blue_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbleu\u001b[39m.\u001b[39;49mcompute(\n\u001b[1;32m     42\u001b[0m         predictions\u001b[39m=\u001b[39;49mgenerated_texts,\n\u001b[1;32m     43\u001b[0m         references\u001b[39m=\u001b[39;49m[[item] \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m original_texts],\n\u001b[1;32m     44\u001b[0m     )[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     46\u001b[0m     \u001b[39m# compute rouge score\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     rougeL_score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/datasets/metric.py:453\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[1;32m    452\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> 453\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/31e1673407d8789b8f5ddfd979948f6a1de0a6d691426d55fa74a35ffb0c1bdf/sacrebleu.py:146\u001b[0m, in \u001b[0;36mSacrebleu._compute\u001b[0;34m(self, predictions, references, smooth_method, smooth_value, force, lowercase, tokenize, use_effective_order)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSacrebleu requires the same number of references for each prediction\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m transformed_references \u001b[39m=\u001b[39m [[refs[i] \u001b[39mfor\u001b[39;00m refs \u001b[39min\u001b[39;00m references] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(references_per_prediction)]\n\u001b[0;32m--> 146\u001b[0m output \u001b[39m=\u001b[39m scb\u001b[39m.\u001b[39;49mcorpus_bleu(\n\u001b[1;32m    147\u001b[0m     predictions,\n\u001b[1;32m    148\u001b[0m     transformed_references,\n\u001b[1;32m    149\u001b[0m     smooth_method\u001b[39m=\u001b[39;49msmooth_method,\n\u001b[1;32m    150\u001b[0m     smooth_value\u001b[39m=\u001b[39;49msmooth_value,\n\u001b[1;32m    151\u001b[0m     force\u001b[39m=\u001b[39;49mforce,\n\u001b[1;32m    152\u001b[0m     lowercase\u001b[39m=\u001b[39;49mlowercase,\n\u001b[1;32m    153\u001b[0m     use_effective_order\u001b[39m=\u001b[39;49muse_effective_order,\n\u001b[1;32m    154\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(\u001b[39mdict\u001b[39;49m(tokenize\u001b[39m=\u001b[39;49mtokenize) \u001b[39mif\u001b[39;49;00m tokenize \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    157\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m: output\u001b[39m.\u001b[39mscore,\n\u001b[1;32m    158\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcounts\u001b[39m\u001b[39m\"\u001b[39m: output\u001b[39m.\u001b[39mcounts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mref_len\u001b[39m\u001b[39m\"\u001b[39m: output\u001b[39m.\u001b[39mref_len,\n\u001b[1;32m    164\u001b[0m }\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m output_dict\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/sacrebleu/compat.py:35\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[0;34m(sys_stream, ref_streams, smooth_method, smooth_value, force, lowercase, tokenize, use_effective_order)\u001b[0m\n\u001b[1;32m     30\u001b[0m args \u001b[39m=\u001b[39m Namespace(\n\u001b[1;32m     31\u001b[0m     smooth_method\u001b[39m=\u001b[39msmooth_method, smooth_value\u001b[39m=\u001b[39msmooth_value, force\u001b[39m=\u001b[39mforce,\n\u001b[1;32m     32\u001b[0m     short\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, lc\u001b[39m=\u001b[39mlowercase, tokenize\u001b[39m=\u001b[39mtokenize)\n\u001b[1;32m     34\u001b[0m metric \u001b[39m=\u001b[39m BLEU(args)\n\u001b[0;32m---> 35\u001b[0m \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39;49mcorpus_score(\n\u001b[1;32m     36\u001b[0m     sys_stream, ref_streams, use_effective_order\u001b[39m=\u001b[39;49muse_effective_order)\n",
      "File \u001b[0;32m~/env/lib/python3.8/site-packages/sacrebleu/metrics/bleu.py:286\u001b[0m, in \u001b[0;36mBLEU.corpus_score\u001b[0;34m(self, sys_stream, ref_streams, use_effective_order)\u001b[0m\n\u001b[1;32m    284\u001b[0m lines \u001b[39m=\u001b[39m [output] \u001b[39m+\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m refs \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lines) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:  \u001b[39m# we need at least hypothesis + 1 defined & non-empty reference\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo valid references for a sentence!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlc:\n\u001b[1;32m    289\u001b[0m     lines \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m lines]\n",
      "\u001b[0;31mEOFError\u001b[0m: No valid references for a sentence!"
     ]
    }
   ],
   "source": [
    "from dimweb_persona_bot.utils import TextEvaluator\n",
    "\n",
    "t_eval = TextEvaluator()\n",
    "\n",
    "t_eval.evaluate(\n",
    "\t[\"\"],\n",
    "\t[\"\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afbd04eaf482342bd8c806741887bf29b8900f429828e19eaba1f287fa9febed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
