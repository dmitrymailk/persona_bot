{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from dimweb_persona_bot.dataloaders.seq2seq_samplers.seq2seq_samplers_hypothesis_2 import H2Seq2SeqInferencePersonaSampleV1\n",
    "from dimweb_persona_bot.hyperparameters.causal_modeling_hyperparameters import H2PersonaChatHyperparametersV1\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "[(history[i], history[i+1]) for i in range(0, len(history), 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm also a graduate student . | i enjoy reading journals and guides related to psychology . | my parents taught me survival skills . | i walk dogs for a living .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" | \".join([\n",
    "            \"i'm also a graduate student .\",\n",
    "            \"i enjoy reading journals and guides related to psychology .\",\n",
    "            \"my parents taught me survival skills .\",\n",
    "            \"i walk dogs for a living .\",\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".split(\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262e2183100d4a62874898c47a5b45a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceab0c6bb9e24543abd873ea0adebe7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07ce5a8288b4d62b278f1a90116c0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b7b3a187894df78f9e40906de5d9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc49805d14234bd2b6813a17e3f2e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26015f63b15f41929b18c2e4e34df87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777419a5342b4659ad7a011cd88a596d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"dim/persona_bot_2_28akcwik\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "hyperparameters = H2PersonaChatHyperparametersV1(\n",
    "\tmodel_name=\"facebook/bart-base\",\n",
    "\tmodel_architecture=\"seq2seq\",\n",
    "\tchat_history_pair_length=7,\n",
    "\tpersona_max_length=14, \n",
    "\tchat_max_length=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anytime. what is your favorite?']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = H2Seq2SeqInferencePersonaSampleV1(\n",
    "\ttokenizer=tokenizer,\n",
    "\thyperparameters=hyperparameters,\n",
    "\tdataset_sample={\n",
    "\t\t\"persona\": [\n",
    "      \t\t'I like chocolate ice cream.', \n",
    "        \t\"Sometimes I feel lonely.\", \n",
    "        \t\"I like to play video games.\"\n",
    "        ],\n",
    "\t\t\"history\": [\n",
    "    \t  \t\"Hi, do you like ice cream?\", \n",
    "       \t\t'i do like ice cream but i prefer chocolate ice cream',\n",
    "\t\t\t\"I feel lonely\",\n",
    "\t\t\t'i feel lonely when i play video games',\n",
    "\t\t\t\"Do you wanna play with me? I think we can have fun together.\",\n",
    "\t\t\t'of course. what else do you like?',\n",
    "\t\t\t\"I like driving simulator games. What about you?\",\n",
    "\t\t\t'i like driving simulator games too',\n",
    "\t\t\t\"Let's play together.\",\n",
    "\t\t\t'okay. what else do you like?',\n",
    "\t\t\t\"When so you wanna play?\",\n",
    "    ]\n",
    "\t}\n",
    ").get_sample()\n",
    "\n",
    "for key in sample.keys():\n",
    "    sample[key] = torch.tensor(sample[key]).unsqueeze(0).to(device)\n",
    "    \n",
    "# model(**sample)\n",
    "answer = model.generate(**sample, \n",
    "\t# do_sample=True, \n",
    "    # top_p=0.95,\n",
    "    penalty_alpha=0.1, \n",
    "    top_k=10\n",
    ")\n",
    "tokenizer.batch_decode(answer, skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dimweb_persona_bot.inference.seq2seq_bots import DialogBotV2\n",
    "\n",
    "bot2 = DialogBotV2(model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    hyperparameters=hyperparameters,\n",
    "    history=[\n",
    "        'Hi, how are you doing?',\n",
    "        \"i'm doing well. how are you?\",\n",
    "        \"I'm fixing a bug right now\",\n",
    "        \"oh wow that's interesting. what bug are you fixing?\",\n",
    "\t],\n",
    "    persona=[\n",
    "        \"I'm a junior frontend developer.\", \n",
    "        'I like racing games.', \n",
    "        'Sometimes I write code for fun.', \n",
    "        \"I'm a computer science fresher.\"\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hi, how are you doing?'\n",
      "i'm doing well, how are you?\n",
      "I'm fixing a bug right now\n",
      "what bug? i'm a developer.\n",
      "Button sometimes turns red, sometimes disappears\n",
      "that's interesting. i'm a computer science major.\n",
      "Can you help me please with it?\n",
      "sure. what do you do for fun?\n",
      "I like to play in Dirt2. It's racing game\n",
      "that's a very popular game.\n",
      "Do you like it?\n",
      "i do. it is very fun.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([\"'Hi, how are you doing?'\", \"i'm doing well, how are you?\", \"I'm fixing a bug right now\", \"what bug? i'm a developer.\", 'Button sometimes turns red, sometimes disappears', \"that's interesting. i'm a computer science major.\", 'Can you help me please with it?', 'sure. what do you do for fun?', \"I like to play in Dirt2. It's racing game\", \"that's a very popular game.\", 'Do you like it?', 'i do. it is very fun.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': [\"i'm also a graduate student .\",\n",
       "  'i enjoy reading journals and guides related to psychology .',\n",
       "  'my parents taught me survival skills .',\n",
       "  'i walk dogs for a living .'],\n",
       " 'history': ['hey how are you today',\n",
       "  \"i'm busy studying psychology for my graduate school class . you ?\",\n",
       "  'i have retired and now spend my time as a pro gambler',\n",
       "  'sounds cool ! i could be your dog walker when you are busy .',\n",
       "  'well being retired i have a lot of spare time either in a casino or outdoors',\n",
       "  'i love the outdoors . i can survive in the wilderness for weeks',\n",
       "  'yeah me to just got back from two weeks in the bush',\n",
       "  'did your parents teach you to survive like mine did ?',\n",
       "  'my dad did he has his own outdoor show its possible you have seen it',\n",
       "  'i may have . what is it called ?',\n",
       "  'jim shockey he has survived in places all over the world',\n",
       "  'i love that show ! i watch it between studying .',\n",
       "  \"yeah didn't get to see him much as a child\",\n",
       "  'that must have been hard . my parents were always hiking with me .'],\n",
       " 'sample_id': '15_7'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dimweb_persona_bot.dataloaders.persona_chat_dataloaders import PersonaChatDatasetV1\n",
    "dataset = PersonaChatDatasetV1(input_dataset_path=\"./datasets/persona_chat/valid.json\")\n",
    "\n",
    "dataset[104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm a senior frontend developer.\n",
      "['Sometimes I write code for fun.', \"I'm a junior frontend developer.\", \"I'm a computer science fresher.\", 'I like racing games.']\n",
      "['Hi, how are you doing?', \"i'm doing great. just got home from a long day of work.\", 'Where do you work?', 'i work in the computer sciences department at the university of louisiana.']\n"
     ]
    }
   ],
   "source": [
    "response = bot2.single_chat(\n",
    "\t'That is cool. What do you do on this work?',\n",
    ")\n",
    "print(response)\n",
    "print(bot2.persona)\n",
    "print(bot2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, how are you doing?',\n",
       " \"i'm doing well. how are you?\",\n",
       " \"I'am fixing a bug right now\",\n",
       " \"oh wow that's interesting. what bug are you fixing?\",\n",
       " \"i'm fixing a computer that needs fixed\",\n",
       " \"oh wow that's pretty cool. what do you do for fun?\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i'm doing well. how are you?\n",
      "Bot: oh wow that's interesting. what bug are you fixing?\n",
      "Bot: i like red hot chilli peppers\n"
     ]
    }
   ],
   "source": [
    "bot2.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, how are you doing?',\n",
       " \"i'm doing well. how are you?\",\n",
       " \"I'am fixing a bug right now\",\n",
       " \"oh wow that's interesting. what bug are you fixing?\",\n",
       " \"Oh, It's disgusting bug. The button on website sometimes is black and sometimes is red. What do you think?\",\n",
       " 'i like red hot chilli peppers']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot2.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### russian bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dimweb_persona_bot.inference.seq2seq_bots import DialogBotV1, DialogBotV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForSeq2SeqLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./models/28akcwik/6/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSeq2SeqLM' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/28akcwik/6/\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "hyperparameters = H2PersonaChatHyperparametersV1(\n",
    "    chat_history_pair_length=7,\n",
    "    max_response_length=512,\n",
    "    persona_max_length=14,\n",
    "    chat_max_length=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot2 = DialogBotV2(model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    hyperparameters=hyperparameters,\n",
    "    history=[\n",
    "        \n",
    "\t],\n",
    "    persona=[\n",
    "        \"Я студент 4 курса.\",\n",
    "        \"Люблю иногда бухать по пятницам.\",\n",
    "        \"У меня нет девушки.\",\n",
    "        \"Люблю писать код.\",\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Привет, я студент 4 курса. А ты?\n",
      "Bot: А что ты делаешь в свободное время?\n",
      "Bot: Я пишу код. А ты?\n",
      "Bot: В свободное время я играю в футбол. А ты?\n"
     ]
    }
   ],
   "source": [
    "bot2.start_chat()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### own russian dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from dimweb_persona_bot.hyperparameters.causal_modeling_hyperparameters import H2PersonaChatHyperparametersV1\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"./datasets/ru_dialog_dataset_v1_mbart-50/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./models/2uuglxhm/checkpoint-140000\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Привет привет ! Как настроение ? Чем занимаешься ?',\n",
       " 'knowledge': 'Я фотограф. У меня есть собака. Мне нравится фотографировать. Я люблю шоколад. Я слушаю поп-рок.',\n",
       " 'dataset_source': 'RUPersonaChatDatasetV3',\n",
       " 'label': 'Привет! Я фотограф. А ты?',\n",
       " 'sample_id': 'RUPersonaChatDatasetV3_0_1',\n",
       " 'input_ids': [250021,\n",
       "  1509,\n",
       "  38335,\n",
       "  5,\n",
       "  447,\n",
       "  3925,\n",
       "  3549,\n",
       "  59095,\n",
       "  59,\n",
       "  5,\n",
       "  24126,\n",
       "  87813,\n",
       "  38335,\n",
       "  16833,\n",
       "  5,\n",
       "  1509,\n",
       "  81880,\n",
       "  112951,\n",
       "  5,\n",
       "  1509,\n",
       "  28203,\n",
       "  743,\n",
       "  62659,\n",
       "  9,\n",
       "  17329,\n",
       "  5,\n",
       "  1813,\n",
       "  18454,\n",
       "  146038,\n",
       "  38,\n",
       "  5187,\n",
       "  124441,\n",
       "  32,\n",
       "  94321,\n",
       "  33309,\n",
       "  132797,\n",
       "  32,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [250021, 1813, 18454, 38, 1509, 38335, 5, 536, 4789, 32, 2]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет! Отлично, а у тебя?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "\t\"Я фотограф. У меня есть собака. Мне нравится фотографировать. Я люблю шоколад. Я слушаю поп-рок. Привет привет ! Как настроение ? Чем занимаешься ?\", \n",
    "    return_tensors=\"pt\",     \n",
    "    ).input_ids.to(\"cuda\")\n",
    "\n",
    "result = model.generate(\n",
    "\tinput_ids,\n",
    "    penalty_alpha=0.15,\n",
    "    top_k=20,\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(result, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dimweb_persona_bot.inference.seq2seq_bots import DialogBotV3\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "path = \"./models/2uuglxhm/checkpoint-160000\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет как дела? <sep> Привет, отлично, как твои? <sep> Хорошо, расскажи сказку про белого бычка.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Какую?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_bot = DialogBotV3(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    history=[\n",
    "\t\t\"Привет как дела ?\",\n",
    "\t\t'Привет, отлично, как твои?',\n",
    "\t\t\"Хорошо, расскажи сказку про белого бычка.\",\n",
    "\t\t# 'Какую сказку?',\n",
    "\t\t# \"Сказку про белого бычка\",\n",
    "\t\t# 'Какую сказку?',\n",
    "\t\t# \"Ты понимаешь что я говорю ?\",\n",
    "\t\t# 'Да, я понимаю, что ты говоришь.',\n",
    "\t\t# \"Какую компьютерную игру ты любишь?\",\n",
    "\t]\n",
    ")\n",
    "\n",
    "ru_bot.next_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет как дела?',\n",
       " 'Привет, отлично, как твои?',\n",
       " 'Хорошо, расскажи смешную сказку про белого бычка',\n",
       " 'Какую сказку?',\n",
       " 'про белого бычка']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(\n",
    "    tokenizer.batch_encode_plus(\n",
    "\t\t[\n",
    "\t\t\t\"Привет как дела ?\",\n",
    "\t\t\t'Привет, отлично, как твои?',\n",
    "\t\t\t\"Хорошо, расскажи смешную сказку про белого бычка\",\n",
    "\t\t],\n",
    "\t\tadd_special_tokens=False,\n",
    "\t\tmax_length=200,\n",
    "\t)['input_ids'],\n",
    "    add_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## independence code for seq2seq persona generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class H2PersonaChatHyperparametersV1:\n",
    "    \"\"\"\n",
    "    chat_history_pair_length: int - количество пар диалога с конца\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"facebook/bart-base\"\n",
    "    chat_history_pair_length: int = 7\n",
    "\n",
    "    persona_max_length: int = 14\n",
    "    chat_max_length: int = 25\n",
    "\n",
    "    debug_status: int = 0\n",
    "\n",
    "\n",
    "class PersonaChatDatasetSampleV1(TypedDict):\n",
    "    \"\"\"\n",
    "    persona: List[str] - набор предложений фактов персоны\n",
    "    history: List[str] - набор предложений истории переписки\n",
    "    \"\"\"\n",
    "\n",
    "    persona: List[str]\n",
    "    history: List[str]\n",
    "    sample_id: str\n",
    "\n",
    "\n",
    "class H2Seq2SeqInferenceSampleDictV1(TypedDict):\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "\n",
    "\n",
    "class H2Seq2SeqInferenceSampleDictV2(TypedDict):\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def flat_list(list_of_lists: List[List]) -> List:\n",
    "    return list(chain.from_iterable(list_of_lists))\n",
    "\n",
    "\n",
    "class H2Seq2SeqInferencePersonaSampleV1:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_sample: PersonaChatDatasetSampleV1,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        hyperparameters: H2PersonaChatHyperparametersV1,\n",
    "    ) -> None:\n",
    "        self.dataset_sample = dataset_sample\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def add_spaces_after(\n",
    "        self,\n",
    "        items: List[str],\n",
    "    ) -> List[str]:\n",
    "        items = [item + \" \" for item in items]\n",
    "        return items\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self):\n",
    "        if \"t5\" in self.hyperparameters.model_name:\n",
    "            return []\n",
    "\n",
    "        if self.tokenizer.bos_token_id is None:\n",
    "            return []\n",
    "\n",
    "        return [self.tokenizer.bos_token_id]\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        if self.tokenizer.eos_token_id is None:\n",
    "            return []\n",
    "\n",
    "        return [self.tokenizer.eos_token_id]\n",
    "\n",
    "    def add_sep_beetween(self, items: List[str], sep=\" EOS \") -> List[str]:\n",
    "        for i in range(1, len(items)):\n",
    "            items[i] = sep + items[i]\n",
    "\n",
    "        return items\n",
    "\n",
    "    def add_spaces_between(self, items: List[str]) -> List[str]:\n",
    "        items = self.add_spaces_after(items)\n",
    "        items[-1] = items[-1].strip()\n",
    "        return items\n",
    "\n",
    "    def get_sample(self) -> H2Seq2SeqInferenceSampleDictV1:\n",
    "\n",
    "        dialog_history = self.dataset_sample[\"history\"]\n",
    "        dialog_history = dialog_history[-self.hyperparameters.chat_history_pair_length * 2 - 1 :]\n",
    "        dialog_history = self.add_sep_beetween(dialog_history)\n",
    "\n",
    "        persona = self.dataset_sample[\"persona\"]\n",
    "        persona = self.add_sep_beetween(\n",
    "            persona,\n",
    "            sep=\" \",\n",
    "        )\n",
    "\n",
    "        KNOWLEDGE_IDS = self.tokenizer.encode(\n",
    "            \" [KNOWLEDGE] \",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        CONTEXT_IDS = self.tokenizer.encode(\n",
    "            \" [CONTEXT]\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "        encoded_history = self.tokenizer.batch_encode_plus(\n",
    "            dialog_history,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=self.hyperparameters.chat_max_length,\n",
    "        )\n",
    "        encoded_history = flat_list(encoded_history[\"input_ids\"])\n",
    "\n",
    "        encoded_persona = self.tokenizer.batch_encode_plus(\n",
    "            persona,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=self.hyperparameters.persona_max_length,\n",
    "        )\n",
    "\n",
    "        encoded_persona = flat_list(encoded_persona[\"input_ids\"])\n",
    "\n",
    "        input_ids = [\n",
    "            *self.bos_token_id,\n",
    "            *CONTEXT_IDS,\n",
    "            *encoded_history,\n",
    "            *KNOWLEDGE_IDS,\n",
    "            *encoded_persona,\n",
    "            *self.eos_token_id,\n",
    "        ]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        return H2Seq2SeqInferenceSampleDictV1(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "\n",
    "class DialogBotV1:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForSeq2SeqLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        hyperparameters: H2PersonaChatHyperparametersV1,\n",
    "        history: List[str] = None,\n",
    "        persona: List[str] = None,\n",
    "        device: str = \"cuda\",\n",
    "        shuffle_persona: bool = True,\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.device = device\n",
    "        self.shuffle_persona = shuffle_persona\n",
    "\n",
    "        self.debug_status = hyperparameters.debug_status\n",
    "\n",
    "        if history is None:\n",
    "            self.history = []\n",
    "        self.history = history\n",
    "\n",
    "        if persona is None:\n",
    "            self.persona = []\n",
    "        self.persona = persona\n",
    "\n",
    "    def _get_sample(\n",
    "        self,\n",
    "        persona: List[str],\n",
    "        history: List[str],\n",
    "    ) -> H2Seq2SeqInferenceSampleDictV1:\n",
    "        dataset_sample = PersonaChatDatasetSampleV1(\n",
    "            persona=persona,\n",
    "            history=history,\n",
    "        )\n",
    "\n",
    "        sample = H2Seq2SeqInferencePersonaSampleV1(\n",
    "            tokenizer=self.tokenizer,\n",
    "            hyperparameters=self.hyperparameters,\n",
    "            dataset_sample=dataset_sample,\n",
    "        )\n",
    "        sample = sample.get_sample()\n",
    "        print(self.tokenizer.decode(sample['input_ids']))\n",
    "\n",
    "        for key in sample.keys():\n",
    "            sample[key] = torch.tensor(sample[key]).unsqueeze(0).to(self.device)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def next_response(\n",
    "        self,\n",
    "        **generation_params,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        делает предсказание на основе текущей истории\n",
    "        и персоны\n",
    "        \"\"\"\n",
    "\n",
    "        sample = self._get_sample(\n",
    "            persona=self.persona,\n",
    "            history=self.history,\n",
    "        )\n",
    "        answer = self.generate_response(\n",
    "            sample,\n",
    "            **generation_params,\n",
    "        )\n",
    "        answer = self.tokenizer.batch_decode(\n",
    "            answer,\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        self.history.append(answer[0])\n",
    "        return answer[0]\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        sample: H2Seq2SeqInferenceSampleDictV1,\n",
    "        **generation_params,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        generation_params - https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/text_generation\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model.generate(\n",
    "                **sample,\n",
    "                **generation_params,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facebook/nllb-200-distilled-600M\n",
    "# PRETRAINED_MODEL_NAME_OR_PATH = \"./models/23khz8hr/5/\"\n",
    "\n",
    "# facebook/mbart-large-50\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"./models/1zbrflmj/3/\"\n",
    "\n",
    "# sberbank-ai/ruT5-base\n",
    "# PRETRAINED_MODEL_NAME_OR_PATH = \"./models/2y5mzb10/6/\"\n",
    "\n",
    "PAIR_DIALOG_HISTORY_LENGTH = 2\n",
    "\n",
    "# CHAT_MAX_LENGTH for single sentence\n",
    "CHAT_MAX_LENGTH = 25\n",
    "# PERSONA_MAX_LENGTH for single sentence\n",
    "PERSONA_MAX_LENGTH = 19\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tmodel.half()\n",
    "\n",
    "hyperparameters = H2PersonaChatHyperparametersV1(\n",
    "\tchat_history_pair_length=PAIR_DIALOG_HISTORY_LENGTH,\n",
    "\tpersona_max_length=PERSONA_MAX_LENGTH,\n",
    "\tchat_max_length=CHAT_MAX_LENGTH,\n",
    "\tmodel_name=PRETRAINED_MODEL_NAME_OR_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONTEXT] Привет. Что ты любишь лук? [KNOWLEDGE]  Я люблю играть с милыми песиками Я ненавижу лук и броколли</s>\n",
      "Привет.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persona = [\n",
    " \t\"Я люблю играть с милыми песиками\",\n",
    "\t\"Я ненавижу лук и броколли\"\n",
    "]\n",
    "\n",
    "history = [\n",
    "\t\"Привет. Что ты любишь лук?\"\n",
    "]\n",
    "            \n",
    "persona_bot = DialogBotV1(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        hyperparameters=hyperparameters,\n",
    "        history=history,\n",
    "        persona=persona,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "GENERATION_PARAMS = {\n",
    "\t\"max_new_tokens\": 60,\n",
    "\t\"penalty_alpha\": 0.15,\n",
    "\t\"top_k\": 10\n",
    "}\n",
    "response = persona_bot.next_response(\n",
    "\t**GENERATION_PARAMS,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved to /home/kosenko/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/DeepPavlov/mbart-large-50-ru-persona-chat/commit/3598b5ccf26927040669e6fdb04de32c2e0944ee', commit_message='Upload tokenizer', commit_description='', oid='3598b5ccf26927040669e6fdb04de32c2e0944ee', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"DeepPavlov/mbart-large-50-ru-persona-chat\")\n",
    "tokenizer.push_to_hub(\"DeepPavlov/mbart-large-50-ru-persona-chat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afbd04eaf482342bd8c806741887bf29b8900f429828e19eaba1f287fa9febed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
