{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "outputs = model(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32099, 1], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<extra_id_0>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> studies have shown that owning a dog is good for you.</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summarize: studies have shown that owning a dog is good for you</s>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[32099]], 'attention_mask': [[1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([\"<extra_id_0>\"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dimweb_persona_bot.eda.eda_scripts import TokensLengthAnalyzerV1\n",
    "\n",
    "model_list = [\n",
    "    \"t5-small\",\n",
    "    \"t5-base\",\n",
    "    \"facebook/bart-base\",\n",
    "    \"google/t5-v1_1-small\",\n",
    "    \"facebook/blenderbot-400M-distill\",\n",
    "    \"google/long-t5-tglobal-base\",\n",
    "]\n",
    "\n",
    "dataset_analyzer = TokensLengthAnalyzerV1(\n",
    "    train_dataset_path=\"./datasets/persona_chat/train.json\",\n",
    "    valid_dataset_path=\"./datasets/persona_chat/valid.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: facebook/bart-base\n",
      "Model: google/t5-v1_1-small\n",
      "Model: facebook/blenderbot-400M-distill\n",
      "Model: google/long-t5-tglobal-base\n"
     ]
    }
   ],
   "source": [
    "dataset_analyzer.analyze(\n",
    "\tmodels_list=model_list,\n",
    " \tsave_path=\"./dimweb_persona_bot/eda/tokens_length_seq2seq_models.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: persona, Stage: train\n",
      "Average replica length: 16.833333333333332\n",
      "\n",
      "\n",
      "Field: persona, Stage: valid\n",
      "Average replica length: 15.833333333333334\n",
      "\n",
      "\n",
      "Field: history, Stage: train\n",
      "Average replica length: 24.0\n",
      "\n",
      "\n",
      "Field: history, Stage: valid\n",
      "Average replica length: 24.0\n",
      "\n",
      "\n",
      "Field: persona\n",
      "Field len percentile 50: 5.0\n",
      "Field len percentile 95: 5.0\n",
      "Field: history\n",
      "Field len percentile 50: 12.0\n",
      "Field len percentile 95: 14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-small, Max length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-base, Max length: 512\n",
      "Model: facebook/bart-base, Max length: 1024\n",
      "Model: google/t5-v1_1-small, Max length: 512\n",
      "Model: facebook/blenderbot-400M-distill, Max length: 128\n",
      "Model: google/long-t5-tglobal-base, Max length: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "dataset_analyzer.show_info(\n",
    "\tdataset_path=\"./dimweb_persona_bot/eda/tokens_length_seq2seq_models.csv\",\n",
    "\tmodels_list=model_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length persona: 80\n",
      "Max length history: 336\n",
      "Total max length: 439\n"
     ]
    }
   ],
   "source": [
    "max_length_persona = 5 * 16\n",
    "max_length_history = 14 * 24\n",
    "special_tokens_persona = 5+1\n",
    "special_tokens_history = 14+1\n",
    "special_tokens = 2\n",
    "print(f\"Max length persona: {max_length_persona}\")\n",
    "print(f\"Max length history: {max_length_history}\")\n",
    "print(f\"Total max length: {max_length_persona + max_length_history + special_tokens_persona + special_tokens_history + special_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[3, 26, 1]], 'attention_mask': [[1, 1, 1]]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"google/t5-v1_1-small\")\n",
    "tok([\"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.72k/1.72k [00:00<00:00, 642kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:01<00:00, 765kB/s]  \n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 594kB/s]  \n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:01<00:00, 1.16MB/s]\n",
      "Downloading: 100%|██████████| 558M/558M [00:52<00:00, 10.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Привет, как дела?</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Привет, как дела?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eda for RU persona chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dimweb_persona_bot.eda.eda_scripts import TokensLengthAnalyzerV2\n",
    "\n",
    "model_list = [\n",
    "    # \"sberbank-ai/ruT5-base\",\n",
    "    # \"facebook/bart-base\",\n",
    "    # \"facebook/mbart-large-50\",\n",
    "    # \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    # \"facebook/mbart-large-cc25\",\n",
    "    # \"facebook/mbart-large-50-many-to-one-mmt\",\n",
    "    # \"facebook/mbart-large-en-ro\",\n",
    "    # \"facebook/mbart-large-50-one-to-many-mmt\",\n",
    "    \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "]\n",
    "\n",
    "dataset_analyzer = TokensLengthAnalyzerV2(\n",
    "    train_dataset_path=\"./datasets/ru_persona_chat/train.csv\",\n",
    "    valid_dataset_path=\"./datasets/ru_persona_chat/valid.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sberbank-ai/rugpt3medium_based_on_gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ef05afb5e54da7918c375ae65fc3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de1d37dd4e1407cabf4024cb78d9c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aecd083deb14c999eb4e45cde5482a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "dataset_analyzer.analyze(\n",
    "\tmodels_list=model_list,\n",
    " \tsave_path=\"./dimweb_persona_bot/eda/tokens_length_seq2seq_models_ru_persona_chat.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>model</th>\n",
       "      <th>95%</th>\n",
       "      <th>stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persona</td>\n",
       "      <td>sberbank-ai/rugpt3medium_based_on_gpt2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>history</td>\n",
       "      <td>sberbank-ai/rugpt3medium_based_on_gpt2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     field                                   model   95%  stage\n",
       "0  persona  sberbank-ai/rugpt3medium_based_on_gpt2   8.0  train\n",
       "1  history  sberbank-ai/rugpt3medium_based_on_gpt2  38.0  train"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"./dimweb_persona_bot/eda/tokens_length_seq2seq_models_ru_persona_chat.csv\")\n",
    "dataset[dataset['stage'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: persona, Stage: train\n",
      "Average replica length: 8.0\n",
      "\n",
      "\n",
      "Field: persona, Stage: valid\n",
      "Average replica length: 8.0\n",
      "\n",
      "\n",
      "Field: history, Stage: train\n",
      "Average replica length: 39.0\n",
      "\n",
      "\n",
      "Field: history, Stage: valid\n",
      "Average replica length: 38.0\n",
      "\n",
      "\n",
      "Field: persona\n",
      "Field len percentile 50: 5.0\n",
      "Field len percentile 95: 5.0\n",
      "Field: history\n",
      "Field len percentile 50: 14.0\n",
      "Field len percentile 95: 30.0\n",
      "Model: sberbank-ai/ruT5-base, Max length: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "dataset_analyzer.show_info(\n",
    "\tdataset_path=\"./dimweb_persona_bot/eda/tokens_length_seq2seq_models_ru_persona_chat.csv\",\n",
    "\tmodels_list=['sberbank-ai/ruT5-base'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_len = 3\n",
    "persona_len = 5 * 10\n",
    "dialog_len = 3 * 2 * 39\n",
    "\n",
    "total_len = history_len + persona_len + dialog_len\n",
    "total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lengths: train 9512, valid 501\n",
      "Datasets saved.\n"
     ]
    }
   ],
   "source": [
    "from dimweb_persona_bot.datasets_transformers.ru_persona_chat_dataset_transformer import ru_persona_chat_dataset_tranformer_v1\n",
    "\n",
    "ru_persona_chat_dataset_tranformer_v1(\n",
    "\tinitial_dataset_path=\"./datasets/ru_persona_chat/dialogues.tsv\",\n",
    "\toutput_folder=\"./datasets/ru_persona_chat\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afbd04eaf482342bd8c806741887bf29b8900f429828e19eaba1f287fa9febed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
